"""
Integration tests for LLM provider system.

Tests the complete LLM provider system integration following Alfred's testing principles.
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock
import os

from alfred.llm.registry import ModelProviderRegistry, get_model_registry
from alfred.llm.initialization import initialize_ai_providers, get_provider_status
from alfred.llm.providers.base import (
    BaseAIProvider,
    ModelResponse,
    ModelInfo,
    ModelCapability,
    ProviderError,
    ModelNotFoundError,
    AuthenticationError,
)
from alfred.models.alfred_config import AIProvider, AIProviderConfig


class MockFullProvider(BaseAIProvider):
    """Complete mock provider for integration testing."""

    def __init__(self, name: str, api_key: str, **kwargs):
        self.name = name
        self.api_key = api_key
        self.kwargs = kwargs
        self._models = self._create_test_models()

    def _create_test_models(self) -> list:
        """Create test models for this provider."""
        return [
            ModelInfo(
                name=f"{self.name}-fast",
                provider=self.name,
                capabilities=[ModelCapability.TEXT_GENERATION],
                context_window=4096,
                max_output_tokens=1024,
                cost_per_input_token=0.001,
                cost_per_output_token=0.002,
            ),
            ModelInfo(
                name=f"{self.name}-smart",
                provider=self.name,
                capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING, ModelCapability.CODE_GENERATION],
                context_window=32768,
                max_output_tokens=4096,
                cost_per_input_token=0.01,
                cost_per_output_token=0.03,
            ),
        ]

    def generate_content(self, prompt: str, model_name: str, temperature: float = 0.5, max_tokens=None) -> ModelResponse:
        if not self.validate_model(model_name):
            raise ModelNotFoundError(f"Model {model_name} not found")

        return ModelResponse(
            content=f"Generated by {self.name}: {prompt[:50]}...",
            model_name=model_name,
            usage={"input_tokens": 10, "output_tokens": 25, "total_tokens": 35},
            metadata={"provider": self.name, "temperature": temperature},
        )

    def count_tokens(self, text: str, model_name: str) -> int:
        if not self.validate_model(model_name):
            raise ModelNotFoundError(f"Model {model_name} not found")
        return len(text.split())

    def get_available_models(self) -> list:
        return self._models

    def validate_model(self, model_name: str) -> bool:
        return any(model.name == model_name for model in self._models)


class TestProviderRegistryIntegration:
    """Test complete provider registry integration."""

    def test_registry_provider_lifecycle(self):
        """Test complete provider lifecycle in registry."""
        registry = ModelProviderRegistry()

        # Create and register providers
        provider1 = MockFullProvider("provider1", "api-key-1")
        provider2 = MockFullProvider("provider2", "api-key-2")

        registry.register_provider("provider1", provider1)
        registry.register_provider("provider2", provider2)

        # Verify registration
        assert registry.is_provider_registered("provider1")
        assert registry.is_provider_registered("provider2")
        assert len(registry.get_registered_providers()) == 2

        # Test model enumeration
        all_models = registry.get_available_models()
        assert len(all_models) == 4  # 2 models per provider

        model_names = [model.name for model in all_models]
        assert "provider1-fast" in model_names
        assert "provider1-smart" in model_names
        assert "provider2-fast" in model_names
        assert "provider2-smart" in model_names

        # Test provider routing
        assert registry.get_provider_for_model("provider1-fast") == provider1
        assert registry.get_provider_for_model("provider2-smart") == provider2

    def test_registry_model_conflict_resolution(self):
        """Test registry handling of model name conflicts."""
        registry = ModelProviderRegistry()

        # Create providers with conflicting model names
        models1 = [ModelInfo(name="shared-model", provider="provider1", capabilities=[ModelCapability.TEXT_GENERATION], context_window=4096)]

        models2 = [ModelInfo(name="shared-model", provider="provider2", capabilities=[ModelCapability.CODE_GENERATION], context_window=8192)]

        provider1 = MockFullProvider("provider1", "key1")
        provider1._models = models1

        provider2 = MockFullProvider("provider2", "key2")
        provider2._models = models2

        registry.register_provider("provider1", provider1)
        registry.register_provider("provider2", provider2)

        # First provider gets the model name
        assert registry.get_provider_for_model("shared-model") == provider1

        # Second provider gets prefixed access
        assert registry.get_provider_for_model("provider2:shared-model") == provider2

    def test_registry_provider_fallback(self):
        """Test registry fallback to provider validation."""
        registry = ModelProviderRegistry()

        # Create provider that validates models not in its registered list
        provider = MockFullProvider("test", "key")
        provider.validate_model = Mock(return_value=True)

        registry.register_provider("test", provider)

        # Should find provider through validation fallback
        result = registry.get_provider_for_model("unknown-model")
        assert result == provider
        provider.validate_model.assert_called_once_with("unknown-model")

    def test_registry_no_provider_found(self):
        """Test registry behavior when no provider found."""
        registry = ModelProviderRegistry()

        provider = MockFullProvider("test", "key")
        provider.validate_model = Mock(return_value=False)

        registry.register_provider("test", provider)

        with pytest.raises(ModelNotFoundError):
            registry.get_provider_for_model("nonexistent-model")


class TestInitializationIntegration:
    """Test complete initialization system integration."""

    @pytest.mark.asyncio
    @patch("alfred.llm.initialization.ConfigManager")
    @patch("alfred.llm.initialization.model_registry")
    @patch("alfred.llm.initialization.settings")
    async def test_full_initialization_flow(self, mock_settings, mock_registry, mock_config_manager):
        """Test complete initialization flow with multiple providers."""
        # Mock settings
        mock_settings.openai_api_key = "openai-test-key"
        mock_settings.anthropic_api_key = "anthropic-test-key"
        mock_settings.google_api_key = "google-test-key"

        # Mock configuration
        mock_config = Mock()
        mock_ai_config = Mock()

        provider_configs = [
            AIProviderConfig(name=AIProvider.OPENAI, enabled=True),
            AIProviderConfig(name=AIProvider.ANTHROPIC, enabled=True),
            AIProviderConfig(name=AIProvider.GOOGLE, enabled=True),
        ]
        mock_ai_config.providers = provider_configs
        mock_config.ai = mock_ai_config

        mock_config_manager_instance = Mock()
        mock_config_manager_instance.load.return_value = mock_config
        mock_config_manager.return_value = mock_config_manager_instance

        # Mock provider creation and registration
        mock_providers = {
            "openai": MockFullProvider("openai", "openai-test-key"),
            "anthropic": MockFullProvider("anthropic", "anthropic-test-key"),
            "google": MockFullProvider("google", "google-test-key"),
        }

        def create_provider_side_effect(provider_type, **kwargs):
            return mock_providers[provider_type]

        mock_registry.create_provider.side_effect = create_provider_side_effect
        mock_registry.get_available_models.return_value = []

        await initialize_ai_providers()

        # Verify all providers were created with correct parameters
        mock_registry.create_provider.assert_any_call("openai", api_key="openai-test-key", base_url=None)
        mock_registry.create_provider.assert_any_call("anthropic", api_key="anthropic-test-key")
        mock_registry.create_provider.assert_any_call("google", api_key="google-test-key")

        # Verify all providers were registered
        mock_registry.register_provider.assert_any_call("openai", mock_providers["openai"])
        mock_registry.register_provider.assert_any_call("anthropic", mock_providers["anthropic"])
        mock_registry.register_provider.assert_any_call("google", mock_providers["google"])

        assert mock_registry.register_provider.call_count == 3

    @pytest.mark.asyncio
    @patch("alfred.llm.initialization.ConfigManager")
    @patch("alfred.llm.initialization.model_registry")
    @patch("alfred.llm.initialization.settings")
    async def test_partial_initialization_success(self, mock_settings, mock_registry, mock_config_manager):
        """Test initialization with some providers failing."""
        # Mock settings - only OpenAI has key
        mock_settings.openai_api_key = "openai-test-key"
        mock_settings.anthropic_api_key = None
        mock_settings.google_api_key = "google-test-key"

        # Mock configuration
        mock_config = Mock()
        mock_ai_config = Mock()

        provider_configs = [
            AIProviderConfig(name=AIProvider.OPENAI, enabled=True),
            AIProviderConfig(name=AIProvider.ANTHROPIC, enabled=True),  # Will fail
            AIProviderConfig(name=AIProvider.GOOGLE, enabled=True),
        ]
        mock_ai_config.providers = provider_configs
        mock_config.ai = mock_ai_config

        mock_config_manager_instance = Mock()
        mock_config_manager_instance.load.return_value = mock_config
        mock_config_manager.return_value = mock_config_manager_instance

        # Mock provider creation - Google will fail
        def create_provider_side_effect(provider_type, **kwargs):
            if provider_type == "openai":
                return MockFullProvider("openai", kwargs["api_key"])
            elif provider_type == "google":
                raise Exception("Google provider creation failed")
            else:
                return MockFullProvider(provider_type, kwargs["api_key"])

        mock_registry.create_provider.side_effect = create_provider_side_effect
        mock_registry.get_available_models.return_value = []

        # Should not raise exception
        await initialize_ai_providers()

        # Only OpenAI should have been registered (Anthropic fails due to missing key, Google fails during creation)
        mock_registry.register_provider.assert_called_once()
        call_args = mock_registry.register_provider.call_args[0]
        assert call_args[0] == "openai"

    @patch("alfred.llm.initialization.model_registry")
    def test_provider_status_reporting(self, mock_registry):
        """Test provider status reporting functionality."""
        # Mock registry state
        mock_registry.get_registered_providers.return_value = ["openai", "anthropic"]

        mock_models = [
            Mock(name="gpt-4", provider="openai"),
            Mock(name="gpt-3.5-turbo", provider="openai"),
            Mock(name="claude-3-5-sonnet", provider="anthropic"),
            Mock(name="claude-3-haiku", provider="anthropic"),
        ]
        mock_registry.get_available_models.return_value = mock_models

        status = get_provider_status()

        assert status["registered_providers"] == ["openai", "anthropic"]
        assert status["total_models"] == 4
        assert status["models_by_provider"]["openai"] == ["gpt-4", "gpt-3.5-turbo"]
        assert status["models_by_provider"]["anthropic"] == ["claude-3-5-sonnet", "claude-3-haiku"]


class TestEndToEndIntegration:
    """Test complete end-to-end LLM system integration."""

    def test_full_system_workflow(self):
        """Test complete workflow from registration to content generation."""
        # Create fresh registry
        registry = ModelProviderRegistry()

        # Create and register providers
        openai_provider = MockFullProvider("openai", "openai-key")
        anthropic_provider = MockFullProvider("anthropic", "anthropic-key")

        registry.register_provider("openai", openai_provider)
        registry.register_provider("anthropic", anthropic_provider)

        # Test model discovery
        all_models = registry.get_available_models()
        assert len(all_models) == 4

        # Test content generation through registry
        openai_model = "openai-fast"
        anthropic_model = "anthropic-smart"

        # Get providers and generate content
        openai_provider_found = registry.get_provider_for_model(openai_model)
        anthropic_provider_found = registry.get_provider_for_model(anthropic_model)

        assert openai_provider_found == openai_provider
        assert anthropic_provider_found == anthropic_provider

        # Generate content
        openai_response = openai_provider_found.generate_content("Test prompt for OpenAI", openai_model, temperature=0.7)

        anthropic_response = anthropic_provider_found.generate_content("Test prompt for Anthropic", anthropic_model, temperature=0.5)

        # Verify responses
        assert isinstance(openai_response, ModelResponse)
        assert isinstance(anthropic_response, ModelResponse)
        assert openai_response.model_name == openai_model
        assert anthropic_response.model_name == anthropic_model
        assert "openai" in openai_response.content
        assert "anthropic" in anthropic_response.content

    def test_model_capability_routing(self):
        """Test routing models based on capabilities."""
        registry = ModelProviderRegistry()

        # Create providers with different capabilities
        fast_provider = MockFullProvider("fast", "key")
        smart_provider = MockFullProvider("smart", "key")

        # Override models for specific capability testing
        fast_provider._models = [ModelInfo(name="fast-text", provider="fast", capabilities=[ModelCapability.TEXT_GENERATION], context_window=4096)]

        smart_provider._models = [ModelInfo(name="smart-reasoning", provider="smart", capabilities=[ModelCapability.REASONING, ModelCapability.ANALYSIS], context_window=32768)]

        registry.register_provider("fast", fast_provider)
        registry.register_provider("smart", smart_provider)

        # Test capability-based model selection
        all_models = registry.get_available_models()

        text_models = [m for m in all_models if ModelCapability.TEXT_GENERATION in m.capabilities]
        reasoning_models = [m for m in all_models if ModelCapability.REASONING in m.capabilities]

        assert len(text_models) == 1
        assert len(reasoning_models) == 1
        assert text_models[0].name == "fast-text"
        assert reasoning_models[0].name == "smart-reasoning"

    def test_error_propagation(self):
        """Test error propagation through the system."""
        registry = ModelProviderRegistry()

        provider = MockFullProvider("test", "key")
        registry.register_provider("test", provider)

        # Test model not found error
        with pytest.raises(ModelNotFoundError):
            provider_found = registry.get_provider_for_model("test-fast")
            provider_found.generate_content("test", "nonexistent-model")

        # Test provider not found error
        with pytest.raises(ModelNotFoundError):
            registry.get_provider_for_model("nonexistent-model")

    def test_global_registry_integration(self):
        """Test integration with global registry singleton."""
        # Get global registry
        global_registry = get_model_registry()

        # Clear any existing state
        global_registry._providers.clear()
        global_registry._model_to_provider_map.clear()

        # Register provider in global registry
        provider = MockFullProvider("global-test", "key")
        global_registry.register_provider("global-test", provider)

        # Verify persistence across calls
        registry2 = get_model_registry()
        assert registry2 is global_registry
        assert registry2.is_provider_registered("global-test")

        # Test content generation through global registry
        found_provider = global_registry.get_provider_for_model("global-test-fast")
        response = found_provider.generate_content("test", "global-test-fast")

        assert isinstance(response, ModelResponse)
        assert response.model_name == "global-test-fast"


class TestSystemResilience:
    """Test system resilience and error handling."""

    def test_registry_with_failing_provider(self):
        """Test registry resilience with failing provider."""
        registry = ModelProviderRegistry()

        # Create provider that fails during model enumeration
        failing_provider = Mock()
        failing_provider.get_available_models.side_effect = Exception("Model enumeration failed")
        failing_provider.validate_model.return_value = False

        working_provider = MockFullProvider("working", "key")

        # Should not raise exception during registration
        registry.register_provider("failing", failing_provider)
        registry.register_provider("working", working_provider)

        # Should only get models from working provider
        all_models = registry.get_available_models()
        assert len(all_models) == 2  # Only from working provider

        # Registry should still function for working provider
        found_provider = registry.get_provider_for_model("working-fast")
        assert found_provider == working_provider

    def test_initialization_resilience(self):
        """Test initialization system resilience."""
        # This would test initialization with network issues, invalid configs, etc.
        # For now, we verify the structure supports resilient operation
        registry = ModelProviderRegistry()

        # Verify registry starts in clean state
        assert len(registry.get_registered_providers()) == 0
        assert len(registry.get_available_models()) == 0

        # Verify it can handle empty operations gracefully
        with pytest.raises(ModelNotFoundError):
            registry.get_provider_for_model("any-model")

        status = get_provider_status()
        assert status["total_models"] == 0
        assert status["registered_providers"] == []
