### **Adopting LOST and Building from Reality**

The goal is to integrate the LOST framework, rename `SLOT` to `Subtask`, and build the necessary progress-tracking tools that are currently missing.

---
### **TASK: AL-31**

#### **Title:**
Core Models: Implement `Subtask` Model Based on LOST Framework

#### **Context:**
This is the foundational task. We must replace the now-deprecated `SLOT` and `Taskflow` models with a single, superior `Subtask` model that is a direct implementation of the LOST (Location, Operation, Specification, Test) framework. This establishes the new atomic unit of work for the entire system.

#### **Implementation Details:**
This task involves modifying `schemas.py` and `planning_artifacts.py` based on the provided codebase.

#### **Files to Modify:**

1.  **`src/alfred/models/schemas.py` (MODIFY):**
    *   **DELETE** the `Taskflow` Pydantic model.
    *   **FIND and DELETE** the `SLOT` Pydantic model.
    *   **ADD** the new `Subtask` Pydantic model in its place.

    ```python
    # ... (OperationType, Task models remain) ...
    
    # DELETE THE Taskflow CLASS
    
    # DELETE THE SLOT CLASS
    
    # ADD THE NEW Subtask CLASS
    class Subtask(BaseModel):
        """The core, self-contained unit of work, based on the LOST framework."""
        subtask_id: str = Field(description="A unique ID for this Subtask, e.g., 'subtask-1'.")
        title: str = Field(description="A short, human-readable title for the Subtask.")
        
        # L: Location
        location: str = Field(description="The primary file path or directory for the work.")
        
        # O: Operation
        operation: OperationType = Field(description="The type of file system operation.")
        
        # S: Specification
        specification: List[str] = Field(description="An ordered list of procedural steps for the AI to execute.")
        
        # T: Test
        test: List[str] = Field(description="A list of concrete verification steps or unit tests to confirm success.")
    ```

2.  **`src/alfred/models/planning_artifacts.py` (MODIFY):**
    *   Update the `ExecutionPlanArtifact` to use the new `Subtask` model.

    ```python
    # src/alfred/models/planning_artifacts.py
    # ... (imports) ...
    from .schemas import Subtask # <--- CHANGE THIS IMPORT
    
    # ... (other artifact models) ...
    
    class ExecutionPlanArtifact(BaseModel):
        # REPLACE 'slots: List[SLOT]' with the following:
        subtasks: List[Subtask] = Field(description="The ordered list of Subtasks that form the execution plan")
    ```

#### **Acceptance Criteria:**
*   The `schemas.py` file reflects the removal of `Taskflow` and `SLOT` and the addition of `Subtask`.
*   The `ExecutionPlanArtifact` in `planning_artifacts.py` is correctly updated to use `subtasks: List[Subtask]`.
*   The project passes all existing tests that do not depend on the old `SLOT` structure.

---
### **TASK: AL-32**

#### **Title:**
Prompt Engineering: Re-architect `generate_slots` Prompt for LOST Framework

#### **Context:**
With the new `Subtask` model in place, we must re-engineer the final prompt of the `plan_task` tool. This prompt will now teach the AI the LOST framework and instruct it to generate the `ExecutionPlan` as a list of `Subtask` objects, not `SLOT`s.

#### **Implementation Details:**
This task involves a complete replacement of the `generate_slots.md` prompt template.

#### **Files to Modify:**

1.  **`src/alfred/templates/prompts/plan_task/generate_slots.md` (REPLACE):**
    *   Replace the entire content of this file with the new, LOST-focused prompt.

    ```markdown
    # ROLE: {{ persona.name }}, {{ persona.title }}
    # TOOL: `alfred.plan_task`
    # TASK: {{ task.task_id }}
    # STATE: generate_slots

    The detailed implementation design has been approved. Your final task is to convert this design into a machine-executable `ExecutionPlan` composed of `Subtask`s, using the **LOST framework**.

    ## The LOST Framework: Your Guiding Principle
    LOST (Location, Operation, Specification, Test) is a structured method for creating unambiguous, verifiable work units. For each item in the approved design, you will generate one or more `Subtask`s. Each `Subtask` MUST contain:
    - **L (Location):** The file path.
    - **O (Operation):** The action (`CREATE`, `MODIFY`, etc.).
    - **S (Specification):** A detailed, numbered list of procedural steps.
    - **T (Test):** A list of concrete steps to verify the subtask was completed correctly.

    **Approved Design:**
    ```json
    {{ additional_context.design_artifact | tojson(indent=2) }}
    ```

    ---
    ### **Directive: Generate Subtasks**
    Translate the approved design into a list of `Subtask` objects. Be precise, be thorough.

    ---
    ### **Required Action**
    Call `alfred.submit_work` with an `ExecutionPlanArtifact`.

    **Required Artifact Structure:**
    ```json
    {
        "subtasks": [
            {
                "subtask_id": "subtask-1",
                "title": "Add TUI Dependencies",
                "location": "pyproject.toml",
                "operation": "MODIFY",
                "specification": [
                    "1. Locate the [project.dependencies] array in pyproject.toml.",
                    "2. Add 'rich' and 'textual' to the list."
                ],
                "test": [
                    "Run 'pip install -e .' and confirm no errors.",
                    "Run 'python -c \"import rich, textual\"' and confirm no import errors."
                ]
            }
        ]
    }
    ```
    ```

#### **Acceptance Criteria:**
*   The `generate_slots.md` prompt is updated to instruct the AI on the LOST framework and the new `Subtask` structure.

---
### **TASK: AL-33**

#### **Title:**
Tools: **Create** `mark_subtask_complete` Progress Tracking Tool

#### **Context:**
Our system currently **lacks** a mechanism for the AI to report progress during the implementation phase. This task is to **create from scratch** the `mark_subtask_complete` tool, which will serve as the essential checkpointing mechanism.

#### **Implementation Details:**
This task involves creating a new tools file and registering the new tool on the server.

#### **Files to Modify/Create:**

1.  **`src/alfred_new/tools/progress.py` (NEW FILE):**
    ```python
    # src/alfred_new/tools/progress.py
    from src.alfred.models.schemas import ToolResponse
    from src.alfred.orchestration.orchestrator import orchestrator
    from src.alfred.state.manager import state_manager
    from src.alfred.core.workflow import ImplementTaskTool
    from src.alfred.lib.logger import get_logger

    logger = get_logger(__name__)

    def mark_subtask_complete_impl(task_id: str, subtask_id: str) -> ToolResponse:
        if task_id not in orchestrator.active_tools:
            return ToolResponse(status="error", message=f"No active tool found for task '{task_id}'.")
        
        active_tool = orchestrator.active_tools[task_id]
        if not hasattr(active_tool, 'completed_subtasks'):
            logger.error(f"Tool for task {task_id} does not support subtask tracking.")
            return ToolResponse(status="error", message="This tool does not support subtask tracking.")
            
        if subtask_id not in active_tool.completed_subtasks:
            active_tool.completed_subtasks.append(subtask_id)
            state_manager.save_tool_state(task_id, active_tool) # Checkpoint progress
            logger.info(f"Subtask '{subtask_id}' for task '{task_id}' marked as complete.")
            return ToolResponse(status="success", message=f"Acknowledged: Subtask '{subtask_id}' is complete.")
        else:
            return ToolResponse(status="success", message=f"Acknowledged: Subtask '{subtask_id}' was already complete.")
    ```

2.  **`src/alfred_new/server.py` (MODIFY):**
    *   Add the new import: `from src.alfred_new.tools.progress import mark_subtask_complete_impl`.
    *   Add the new tool definition.

    ```python
    # ... (existing imports and tools) ...

    @app.tool()
    async def mark_subtask_complete(task_id: str, subtask_id: str) -> ToolResponse:
        """
        Marks a single Subtask as complete during the implementation phase.

        This tool is the primary checkpointing mechanism. The AI agent MUST
        call this tool after successfully executing each Subtask defined in the
        Execution Plan. This allows Alfred to track progress and ensures a
        resilient, auditable workflow.

        Args:
            task_id: The unique identifier for the parent task.
            subtask_id: The unique identifier of the Subtask being marked complete.
        """
        # ... (standard request/response logging boilerplate) ...
        response = mark_subtask_complete_impl(task_id, subtask_id)
        # ...
        return response
    ```
3. **`src/alfred_new/core/workflow.py` (MODIFY):**
    *   Find the (not-yet-created) `ImplementTaskTool` class definition placeholder and add the `completed_subtasks` attribute to its `__init__`.
    *   *Self-correction:* Since we haven't created `ImplementTaskTool` yet, this is a note for its future implementation. We will add `self.completed_subtasks: List[str] = []` to it when we build it. This task is self-contained.

#### **Acceptance Criteria:**
*   The `mark_subtask_complete` tool is created, implemented, and exposed on the MCP server.
*   The tool correctly interfaces with the (future) `ImplementTaskTool`'s state.

---
### **TASK: AL-34**

#### **Title:**
Retrofit: Enforce `mark_subtask_complete` Reporting in `plan_task`

#### **Context:**
This final, critical task ensures that the plans generated by `plan_task` are compatible with our new progress-tracking system. We will modify the `generate_slots` prompt to explicitly instruct the AI to embed the call to `mark_subtask_complete` within the `Specification` of every `Subtask` it creates.

#### **Implementation Details:**
This is a targeted modification to a single prompt template.

#### **Files to Modify:**

1.  **`src/alfred_new/templates/prompts/plan_task/generate_slots.md` (MODIFY):**
    *   Find the section about the `Specification`.
    *   Add a new, explicit directive.

    ```markdown
    # ... (existing content of the new prompt from AL-32) ...
    
    ## The LOST Framework: Your Guiding Principle
    ...
    - **S (Specification):** A detailed, numbered list of procedural steps.
    - **T (Test):** A list of concrete steps to verify the subtask was completed correctly.

    **CRITICAL REPORTING PROTOCOL:**
    For every `Subtask` you generate, the **final step** in its `specification` list **MUST** be an instruction to report completion. For example:
    `"Call `alfred.mark_subtask_complete` with the `subtask_id` for this Subtask."`

    **Approved Design:**
    ...
    ```

#### **Acceptance Criteria:**
*   When `plan_task` is run, the generated `ExecutionPlan`'s `Subtask`s now all contain the required `mark_subtask_complete` call as their final specification step.

---
### **Execution & Parallelization Strategy**

*   **Sequential Block 1 (Core Models):** `AL-31` must be done first.
*   **Parallel Block 2 (Can be done concurrently after Block 1):**
    *   `AL-32` (Prompt Engineering for `plan_task`)
    *   `AL-33` (Creating the new `progress` tool)
*   **Sequential Block 3 (Final Integration):** `AL-34` should be done after `AL-32` to ensure the base prompt is correct before adding the final directive.

