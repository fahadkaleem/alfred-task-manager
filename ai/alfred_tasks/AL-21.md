#### **Title:**
Core Infrastructure: Fix Logging and Implement Human-Readable Scratchpad

#### **Context:**
Our current implementation has two critical defects that hinder usability and debugging. First, task-specific logging is not being initiated, leaving us without crucial debug information. Second, the `scratchpad.md` is being populated with raw JSON, making it unreadable for humans. This task will fix both issues. We will integrate logging into the start of every tool's lifecycle and refactor the `ArtifactManager` and `submit_work` tool to use Jinja2 templates for generating clean, human-readable markdown artifacts in the scratchpad.

#### **Implementation Details:**

**Part 1: Fix Task-Specific Logging**

1.  **Refactor `plan_task_impl` (and all future tool controllers):** The `plan_task_impl` function in `src/alfred/tools/plan_task.py` is the entry point for the tool. This is the correct place to initiate logging. Add a call to `setup_task_logging(task_id)` at the very beginning of this function.
2.  **Implement Cleanup Logic:** When a tool's lifecycle completes (in the `provide_review_impl` "Terminal State" block), we must add a call to `cleanup_task_logging(task_id)` to close the file handler and clean up resources.

**Part 2: Implement Human-Readable Scratchpad**

1.  **Create Artifact Templates:** In `src/alfred/templates/artifacts/`, we will create new `.md` templates for each artifact. These templates will define how a given artifact (e.g., `ContextAnalysisArtifact`) is rendered into beautiful, readable markdown.
2.  **Refactor `ArtifactManager`:** The `append_to_scratchpad` method must be refactored. It will now take `task_id`, `state_name`, the validated `artifact_model`, and the `persona_config` as arguments. It will then dynamically find the correct template (e.g., `templates/artifacts/context_analysis.md`) and render it using the artifact data.
3.  **Refactor `submit_work_impl`:** The `submit_work` tool will no longer render anything itself. After validating the artifact, it will simply call the new `artifact_manager.append_to_scratchpad` with the necessary data.

**Dev Notes:**
*   This establishes a clean pattern: `tools` contain logic, `prompter` generates prompts (what to do next), and `artifact_manager` renders artifacts (what was just done). This is a robust separation of concerns.
*   The artifact template names should correspond to the artifact model names for easy, dynamic lookup (e.g., `ContextAnalysisArtifact` -> `context_analysis.md`).

**Files to Modify/Create:**

1.  **`src/alfred/tools/plan_task.py` (MODIFY `plan_task_impl`):**
    ```python
    # src/alfred/tools/plan_task.py
    # ... imports ...
    from src.alfred.lib.logger import setup_task_logging

    async def plan_task_impl(task_id: str) -> ToolResponse:
        """Implementation logic for the plan_task tool."""
        # --- ADD LOGGING INITIATION ---
        setup_task_logging(task_id)

        task = load_task(task_id)
        # ... rest of the function remains the same ...
    ```

2.  **`src/alfred/tools/provide_review.py` (MODIFY `provide_review_impl`):**
    ```python
    # src/alfred/tools/provide_review.py
    # ... imports ...
    from src.alfred.lib.logger import cleanup_task_logging
    
    def provide_review_impl(...):
        # ... logic ...
        if active_tool.is_terminal:
            # ... update status, remove from orchestrator ...
            
            # --- ADD LOGGING CLEANUP ---
            cleanup_task_logging(task_id)
            
            logger.info(f"Tool '{active_tool.tool_name}' for task {task_id} completed...")
            # ... rest of completion logic ...
    ```

3.  **`src/alfred/templates/artifacts/context_analysis.md` (NEW FILE):**
    ```markdown
    ### Context Analysis for `{{ task.task_id }}`
    *State: `contextualize`*

    **Analysis Summary:**
    {{ artifact.context_summary }}

    **Identified Affected Files:**
    {% for file in artifact.affected_files %}
    - `{{ file }}`
    {% endfor %}

    **Questions for Developer:**
    {% for question in artifact.questions_for_developer %}
    - {{ question }}
    {% endfor %}
    ```

4.  **Create similar templates** for `strategy.md` and `design.md` in `src/alfred/templates/artifacts/`.

5.  **`src/alfred/lib/artifact_manager.py` (REFACTOR `append_to_scratchpad`):**
    ```python
    # src/alfred/lib/artifact_manager.py
    # ... imports ...
    
    class ArtifactManager:
        def __init__(self):
            # ... jinja env setup remains the same ...
        
        def append_to_scratchpad(self, task_id: str, state_name: str, artifact: BaseModel, persona_config: dict):
            """Renders a structured artifact to markdown and appends it to the scratchpad."""
            scratchpad_path = self._get_scratchpad_path(task_id)
            
            # Dynamically determine template name from artifact class name
            # e.g., ContextAnalysisArtifact -> context_analysis.md
            artifact_type_name = artifact.__class__.__name__
            template_name_snake = re.sub(r'(?<!^)(?=[A-Z])', '_', artifact_type_name).lower().replace('_artifact', '')
            template_path = f"artifacts/{template_name_snake}.md"

            try:
                template = self.jinja_env.get_template(template_path)
            except Exception as e:
                logger.error(f"Could not find artifact template '{template_path}': {e}. Falling back to raw JSON.")
                rendered_content = f"### Submission for State: `{state_name}`\n\n```json\n{artifact.model_dump_json(indent=2)}\n```"
            else:
                context = {
                    "task": load_task(task_id), # Load task for context
                    "state_name": state_name,
                    "artifact": artifact,
                    "persona": persona_config
                }
                rendered_content = template.render(context)

            with scratchpad_path.open("a", encoding="utf-8") as f:
                if scratchpad_path.stat().st_size > 0:
                    f.write("\n\n---\n\n")
                f.write(rendered_content)
            logger.info(f"Appended rendered artifact '{template_name_snake}' to scratchpad for task {task_id}")

    # ... other methods ...
    ```

6.  **`src/alfred/tools/submit_work.py` (REFACTOR `submit_work_impl`):**
    ```python
    # src/alfred/tools/submit_work.py
    # ... imports ...

    def submit_work_impl(task_id: str, artifact: dict) -> ToolResponse:
        # ... (find active_tool, task, validate artifact as before) ...
        # The validated_artifact is now a Pydantic model instance

        # --- REFACTORED: Artifact Persistence ---
        # The tool no longer renders anything. It just passes the validated
        # artifact model to the manager.
        artifact_manager.append_to_scratchpad(
            task_id=task_id,
            state_name=active_tool.state.value,
            artifact=validated_artifact,
            persona_config=load_persona(active_tool.persona_name)
        )
        
        # ... (rest of the function: state transition, prompt generation) ...
    ```

#### **Acceptance Criteria:**
*   When `plan_task` is called for a new task, a `debug/TS-XX/alfred.log` file is created.
*   When `submit_work` is called, the `scratchpad.md` file is appended with beautifully formatted markdown, not a raw JSON block.
*   The content of the markdown in the scratchpad is generated from a template in the `templates/artifacts/` directory.
*   When the `plan_task` tool fully completes, the `cleanup_task_logging` function is called.

#### **AC Verification:**
*   Stop and ask the user to restart the MCP server, once they do then use the mcp to call the tools 
*   Delete the `.alfred/workspace/TS-01` and `.alfred/debug/TS-01` directories to ensure a clean run.
*   Run the `plan_task("TS-01")` and `submit_work(...)` sequence again.
*   **Assert** that a log file now exists at `.alfred/debug/TS-01/alfred.log`.
*   **Inspect** the `.alfred/workspace/TS-01/scratchpad.md` file and confirm its content is formatted markdown.
