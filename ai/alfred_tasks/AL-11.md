### **TASK: AL-11**

#### **Title:**
Tool Implementation: Build `submit_work` and `provide_review` Engine

#### **Context:**
With the testing harness in place, we will now build the engine that drives the conversational loop for all our workflow tools. This involves implementing the two generic, context-aware toolsâ€”`submit_work` and `provide_review`. These tools are the heart of our interactive system. They will look up the currently active workflow tool for a given task, trigger the correct event on its internal State Machine, persist artifacts, and use the `Prompter` to generate the next prompt in the conversation.

#### **CRITICAL: DO NOT USE MOCKING IN TESTS**
All tests for these tools must use the `alfred_test_project` fixture. We will test the real file I/O and state transitions.

#### **Implementation Details:**
1.  **Create `submit_work.py` and `provide_review.py` modules** in `src/alfred/tools/`.
2.  **Implement `submit_work_impl` function.** This logic is the core of submitting an artifact and transitioning to a review state.
3.  **Implement `provide_review_impl` function.** This logic handles the approval/rejection cycle and, critically, the **tool completion and handoff** when a terminal state is reached.

**Files to Create/Modify:**

1.  **`src/alfred/tools/submit_work.py` (NEW FILE):**
    ```python
    # src/alfred/tools/submit_work.py
    from src.alfred.models.schemas import ToolResponse, Task
    from src.alfred.orchestration.orchestrator import orchestrator
    from src.alfred.lib.task_utils import load_task
    from src.alfred.lib.artifact_manager import artifact_manager
    from src.alfred.core.prompter import prompter
    from src.alfred.lib.persona_loader import load_persona

    async def submit_work_impl(task_id: str, artifact: dict) -> ToolResponse:
        if task_id not in orchestrator.active_tools:
            return ToolResponse(status="error", message=f"No active tool found for task '{task_id}'.")
        
        active_tool = orchestrator.active_tools[task_id]
        task = load_task(task_id)
        if not task:
            return ToolResponse(status="error", message=f"Task '{task_id}' not found.")

        # --- Artifact Persistence ---
        # We need a way to render the artifact for the scratchpad.
        # Let's assume a simple rendering for now.
        rendered_artifact = f"# Artifact for state: {active_tool.state}\n\n```json\n{json.dumps(artifact, indent=2)}\n```"
        artifact_manager.append_to_scratchpad(task_id, rendered_artifact)
        
        # --- State Transition ---
        current_state_val = active_tool.state.value
        trigger = f"submit_{current_state_val}"
        
        if not hasattr(active_tool, trigger):
            return ToolResponse(status="error", message=f"Invalid action: cannot submit from state '{current_state_val}'.")
            
        getattr(active_tool, trigger)() # e.g., call tool.submit_contextualize()
        
        # --- Generate Next Prompt ---
        persona_config = load_persona(active_tool.persona_name) # Requires persona_name on tool
        next_prompt = prompter.generate_prompt(
            task=task,
            tool_name=active_tool.tool_name, # Requires tool_name on tool
            state=active_tool.state,
            persona_config=persona_config
        )

        return ToolResponse(status="success", message="Work submitted for review.", next_prompt=next_prompt)
    ```
2.  **`src/alfred/tools/provide_review.py` (NEW FILE):** (Create with similar logic, handling `ai_approve`/`request_revision` and the terminal state check).
3.  **Refactor `BaseWorkflowTool`:** Add `tool_name` and `persona_name` attributes to the base class so `submit_work` and `provide_review` can access them.

#### **Acceptance Criteria & AC Verification:**
*   To be tested in the next task (`AL-12`) via a full integration test of the `plan_task` lifecycle.

#### **Unit Tests:**
*   Create `tests/tools/test_submit_work.py` and `tests/tools/test_provide_review.py`.
*   Write integration tests using the `alfred_test_project` harness.
    *   The tests will first call `plan_task_impl` to set up an active tool.
    *   Then, they will call `submit_work_impl` and assert that the tool's state has changed and the scratchpad file contains the new artifact.
    *   They will then call `provide_review_impl` and assert the subsequent state changes and eventual tool completion.
