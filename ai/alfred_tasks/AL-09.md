### **TASK: AL-09**

#### **Title:**
Test Refactoring: Rewrite `plan_task` Tests Using the "Simulated Reality" Harness

#### **Context:**
The previous tests submitted for the `plan_task` tool relied heavily on mocking, which violates our core testing philosophy. Mocking creates brittle tests that do not validate the true, end-to-end behavior of our system. We will now rewrite these tests using the `alfred_test_project` fixture defined in `tests/conftest.py`. This "Simulated Reality" approach ensures we are testing the tool's real-world interaction with the file system, state management, and other components, providing a much higher degree of confidence in our code.

#### **CRITICAL: DO NOT USE MOCKING**
You are explicitly forbidden from using `unittest.mock.patch` or any other mocking library to simulate the behavior of `load_task`, `save_task`, `update_task_status`, the `Orchestrator`, or the `Prompter`. You will use the `alfred_test_project` fixture to create a real, temporary project on the file system and test against that.

#### **Implementation Details:**
1.  **Locate the test file:** The work will be done in `tests/tools/test_plan_task.py`.
2.  **Delete existing tests:** Remove all existing tests within the `TestPlanTaskTool` class.
3.  **Import the `AlfredTestProject` fixture.**
4.  **Rewrite tests using the fixture.** Each test will now accept `alfred_test_project: AlfredTestProject` as an argument. The test logic will follow the "Arrange, Act, Assert" pattern using the fixture's helper methods.

**Dev Notes:**
*   This is the standard we will use for all tool testing going forward.
*   The `alfred_test_project` fixture handles all setup and teardown, including creating temporary directories and changing the current working directory. The tests themselves only need to focus on the logic.
*   Assertions will be made against the *actual state of the file system* within the temporary project, not against mock call counts.

**Files to Modify:**

1.  **`tests/tools/test_plan_task.py` (REWRITE):**
    ```python
    # tests/tools/test_plan_task.py
    import pytest
    from pathlib import Path
    
    from src.alfred.models.schemas import Task, TaskStatus, ToolResponse
    from src.alfred.tools.plan_task import plan_task_impl
    from src.alfred.orchestration.orchestrator import orchestrator
    from tests.conftest import AlfredTestProject # Import the fixture helper

    @pytest.mark.asyncio
    async def test_plan_task_success(alfred_test_project: AlfredTestProject):
        """
        Tests the full success path of plan_task using a real file system.
        NO MOCKING.
        """
        # --- ARRANGE ---
        # 1. Initialize a real project in a temporary directory
        alfred_test_project.initialize()
        
        # 2. Create a dummy persona and prompt template file
        (alfred_test_project.alfred_dir / "personas").mkdir(exist_ok=True)
        (alfred_test_project.alfred_dir / "personas" / "planning.yml").write_text("name: Alex")
        (alfred_test_project.alfred_dir / "templates" / "prompts" / "plan_task").mkdir(parents=True, exist_ok=True)
        (alfred_test_project.alfred_dir / "templates" / "prompts" / "plan_task" / "contextualize.md").write_text("Prompt for {{ task.title }}")

        # 3. Create a valid Task object and save it to the test project's file system
        task = Task(task_id="AL-01", title="My Test Task", context="...", implementation_details="...")
        alfred_test_project.create_task_file(task)

        # --- ACT ---
        # 4. Run the actual tool implementation
        result: ToolResponse = await plan_task_impl("AL-01")

        # --- ASSERT ---
        # 5. Assert the tool response is correct
        assert result.status == "success"
        assert "Planning initiated" in result.message
        assert result.next_prompt == "Prompt for My Test Task"

        # 6. Assert against the real file system state
        updated_task = alfred_test_project.load_task("AL-01")
        assert updated_task is not None
        assert updated_task.task_status == TaskStatus.PLANNING

        # 7. Assert against the real orchestrator state
        assert "AL-01" in orchestrator.active_tools
        assert orchestrator.active_tools["AL-01"].task_id == "AL-01"

    @pytest.mark.asyncio
    async def test_plan_task_invalid_status(alfred_test_project: AlfredTestProject):
        """
        Tests that plan_task correctly fails if the task is not in 'new' state.
        NO MOCKING.
        """
        # --- ARRANGE ---
        alfred_test_project.initialize()
        task = Task(
            task_id="AL-02", 
            title="A Task In Progress", 
            context="...", 
            implementation_details="...",
            task_status=TaskStatus.IN_DEVELOPMENT # Invalid initial state
        )
        alfred_test_project.create_task_file(task)

        # --- ACT ---
        result: ToolResponse = await plan_task_impl("AL-02")

        # --- ASSERT ---
        assert result.status == "error"
        assert "status 'in_development'" in result.message

    @pytest.mark.asyncio
    async def test_plan_task_not_found(alfred_test_project: AlfredTestProject):
        """
        Tests that plan_task correctly fails if the task file does not exist.
        NO MOCKING.
        """
        # --- ARRANGE ---
        alfred_test_project.initialize()

        # --- ACT ---
        result: ToolResponse = await plan_task_impl("AL-99") # This task does not exist

        # --- ASSERT ---
        assert result.status == "error"
        assert "Task 'AL-99' not found" in result.message
    ```

#### **Acceptance Criteria:**
*   The `tests/tools/test_plan_task.py` file is rewritten to use the `alfred_test_project` fixture.
*   All `unittest.mock.patch` calls are removed from the file.
*   The tests correctly set up data on the temporary file system (e.g., creating `task.json`, `planning.yml`).
*   The tests call the real `plan_task_impl` function.
*   The tests make assertions based on the `ToolResponse` object and by reading the state of files back from the temporary file system.

#### **AC Verification:**
*   Run `pytest tests/tools/test_plan_task.py`.
*   Confirm that all tests pass.
*   Set the `ALFRED_KEEP_TEST_DATA=1` environment variable and re-run the tests.
*   Inspect the `tests/temp_data/` directory and confirm that subdirectories for each test case were created, each containing a `.alfred-test` workspace. This proves the harness is working as designed.